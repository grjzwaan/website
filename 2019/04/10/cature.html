<p>During a spare hour I decided to see how fast I could prototype something with the fast.ai library. The library is impressive, I can’t say otherwise. Very quickly I got a working prototype. The result was a fairly decent model to recognize the posture of a cat.</p>

<!--more-->
<h2 id="fastai-library">Fast.ai library</h2>
<p>Recently I’ve decided to take a look at <a href="https://course.fast.ai">fast.ai</a>, their course and their library. The best way to do this is to try something and it’s very likely you’ll end up with a cat-related topic. With my desk oriented towards the spot my cat was chilling, I decided to take the library for a spin and recognize cat postures.</p>

<p>To make the post self-contained I’ll note the snippets of code needed. As you’ll see it’s actually very little code - and every line is clear on what it does.</p>

<p>Note: Most of the code was copied from the example notebook in the course of fast.ai.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">fastai.vision</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastai.metrics</span> <span class="kn">import</span> <span class="n">error_rate</span>
</code></pre></div></div>

<h2 id="the-postures">The postures</h2>
<p>I picked the first four postures that came to mind:</p>

<ul>
  <li>Lounging</li>
  <li>Curled up</li>
  <li>Sitting</li>
  <li>Walking</li>
</ul>

<p><img src="postures.png" alt="Example images of cats with different postures." /></p>

<h2 id="getting-the-data">Getting the data</h2>
<p>There are some pictures of my cat but not sufficient. He is very lazy and is predominantly curled up.
In the course of fast.ai they discussed the option to just use Google searches to get a dataset quickly. I have done this before, but they had some nice helpers to really make it a smooth experience:</p>

<p>1) Do a Google Image search yourself. For example, search for “cats curled up”</p>

<p>2) Run a JavaScript script they supply and obtain a file with urls</p>
<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">urls</span> <span class="o">=</span> <span class="nb">Array</span><span class="p">.</span><span class="k">from</span><span class="p">(</span><span class="nb">document</span><span class="p">.</span><span class="nx">querySelectorAll</span><span class="p">(</span><span class="dl">'</span><span class="s1">.rg_di .rg_meta</span><span class="dl">'</span><span class="p">)).</span><span class="nx">map</span><span class="p">(</span><span class="nx">el</span><span class="o">=&gt;</span><span class="nx">JSON</span><span class="p">.</span><span class="nx">parse</span><span class="p">(</span><span class="nx">el</span><span class="p">.</span><span class="nx">textContent</span><span class="p">).</span><span class="nx">ou</span><span class="p">);</span>
<span class="nb">window</span><span class="p">.</span><span class="nx">open</span><span class="p">(</span><span class="dl">'</span><span class="s1">data:text/csv;charset=utf-8,</span><span class="dl">'</span> <span class="o">+</span> <span class="nx">escape</span><span class="p">(</span><span class="nx">urls</span><span class="p">.</span><span class="nx">join</span><span class="p">(</span><span class="dl">'</span><span class="se">\n</span><span class="dl">'</span><span class="p">)));</span>
</code></pre></div></div>
<p>3) Run a function from their library to download and verify the images</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">download_images</span><span class="p">(</span><span class="s">'path/urls_cats.csv'</span><span class="p">,</span> <span class="s">'path/where/to/download/images'</span><span class="p">,</span> <span class="n">max_pics</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">verify_images</span><span class="p">(</span><span class="s">'path/where/to/download/images'</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</code></pre></div></div>
<p>4) Create a dataset</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># Set a fixed seed to always get the same train-validation split
</span><span class="n">data</span> <span class="o">=</span> <span class="n">ImageDataBunch</span><span class="p">.</span><span class="n">from_folder</span><span class="p">(</span><span class="s">"path/to/images"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="s">"."</span><span class="p">,</span> <span class="n">valid_pct</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">ds_tfms</span><span class="o">=</span><span class="n">get_transforms</span><span class="p">(),</span> <span class="n">size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">).</span><span class="n">normalize</span><span class="p">(</span><span class="n">imagenet_stats</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="training-the-neural-network">Training the neural network</h2>
<p>Training the neural network is actually very simple</p>

<p>5) Create the neural network:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">models</span><span class="p">.</span><span class="n">resnet50</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">error_rate</span><span class="p">)</span>
</code></pre></div></div>

<p>6) Find a a good learning rate:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span><span class="p">.</span><span class="n">lr_find</span><span class="p">(</span><span class="n">start_lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="n">recorder</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
</code></pre></div></div>

<p>7) Train the neural network:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span><span class="p">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">max_lr</span><span class="o">=</span><span class="mf">2e-2</span><span class="p">)</span>
</code></pre></div></div>
<p>There are some additional steps you can take to further finetune the model, but notice how easy it is?</p>
<h2 id="performance">Performance</h2>
<p>The performance was actually quite decent: an error rate of 17%. Especially if you realize that the the distinction between classes is fuzzy and can be subtle.
The images that the google search returned quite often confused lounging with curled up and sitting with lounging.</p>

<p>Even I’m not totally clear on the distinction between lounging and curled up in some cases :)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">interp</span> <span class="o">=</span> <span class="n">ClassificationInterpretation</span><span class="p">.</span><span class="n">from_learner</span><span class="p">(</span><span class="n">learn</span><span class="p">)</span>
<span class="n">interp</span><span class="p">.</span><span class="n">plot_confusion_matrix</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="confusion.png" alt="Confusion matrix of the results." /></p>

<p>Let’s plot the images the model was the most confused about, and highlight the areas that the model was focused on:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">interp</span><span class="p">.</span><span class="n">plot_top_losses</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">11</span><span class="p">))</span>
</code></pre></div></div>
<p><img src="toploss.png" alt="The examples where the algorithms makes the largest mistakes." /></p>

<h2 id="improvements">Improvements</h2>
<p>As you can see in the top losses, the samples contain some images that are no good: multiple cats or to far zoomed in. The easiest thing to do would be to get a better dataset and make a clear distinction between the classes.</p>

<h2 id="how-does-it-work">How does it work</h2>
<p>The cool thing fast.ai did in their course is to encode as many best practices in the library as they can.
This means that you don’t start with a neural network from scratch: you take a network architecture that is tested in practices, is pretrained and <em>you get sensible defaults</em>.</p>

<p>In my case I used <a href="https://www.kaggle.com/keras/resnet50">ResNet50</a> and then fine-tuned it with the cat data.
This means you’re getting a lot for free!</p>
